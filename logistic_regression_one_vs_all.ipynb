{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Train Data\n",
    "x = []\n",
    "y = []\n",
    "filename = 'cifar-10-batches-py/data_batch_%d'\n",
    "for b in range(1,6):\n",
    "    f = os.path.join(filename  % (b, ))\n",
    "    with open(f, 'r') as f:\n",
    "        datadict = pickle.load(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "    x.append(X)\n",
    "    y.append(Y)    \n",
    "    \n",
    "x_train = np.concatenate(x)\n",
    "y_train = np.concatenate(y)\n",
    "\n",
    "#Load Test Data\n",
    "f = 'cifar-10-batches-py/test_batch'\n",
    "with open(f, 'r') as f:\n",
    "    datadict = pickle.load(f)\n",
    "    x_test = datadict['data']\n",
    "    y_test = datadict['labels']\n",
    "    x_test = x_test.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get RAW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_training = 49000\n",
    "num_val = 1000\n",
    "num_test = 10000\n",
    "\n",
    "# subsample the data for validation set\n",
    "mask = xrange(num_training, num_training + num_val)\n",
    "x_val_raw = x_train[mask]\n",
    "y_val_raw = y_train[mask]\n",
    "\n",
    "mask = xrange(num_training)\n",
    "x_train_raw = x_train[mask]\n",
    "y_train_raw = y_train[mask]\n",
    "\n",
    "mask = xrange(num_test)\n",
    "x_test_raw = x_test[mask]\n",
    "y_test_raw = y_test[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "x_train = np.reshape(x_train_raw, (x_train_raw.shape[0], -1)) # [49000, 3072]\n",
    "x_val = np.reshape(x_val_raw, (x_val_raw.shape[0], -1)) # [1000, 3072]\n",
    "x_test = np.reshape(x_test_raw, (x_test_raw.shape[0], -1)) # [10000, 3072]\n",
    "    \n",
    "# Normalize the data: subtract the mean image\n",
    "mean_image = np.mean(x_train, axis = 0)\n",
    "x_train -= mean_image\n",
    "x_val -= mean_image\n",
    "x_test -= mean_image\n",
    "    \n",
    "# Add bias dimension and transform into columns\n",
    "x_train = np.hstack([x_train, np.ones((x_train.shape[0], 1))]).T\n",
    "x_val = np.hstack([x_val, np.ones((x_val.shape[0], 1))]).T\n",
    "x_test = np.hstack([x_test, np.ones((x_test.shape[0], 1))]).T\n",
    "\n",
    "y_train = y_train_raw\n",
    "y_val = y_val_raw\n",
    "y_test = y_test_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and train 10 logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 1/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.066540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanfilho/.local/lib/python2.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: numpy boolean negative (the unary `-` operator) is deprecated, use the bitwise_xor (the `^` operator) or the logical_xor function instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100/1000: loss 2.001810\n",
      "iteration 200/1000: loss 1.705051\n",
      "iteration 300/1000: loss 1.500079\n",
      "iteration 400/1000: loss 1.364895\n",
      "iteration 500/1000: loss 1.188604\n",
      "iteration 600/1000: loss 1.144279\n",
      "iteration 700/1000: loss 1.008724\n",
      "iteration 800/1000: loss 0.940295\n",
      "iteration 900/1000: loss 0.919573\n",
      "\n",
      "The 2/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.777222\n",
      "iteration 100/1000: loss 1.997938\n",
      "iteration 200/1000: loss 1.734254\n",
      "iteration 300/1000: loss 1.471140\n",
      "iteration 400/1000: loss 1.420673\n",
      "iteration 500/1000: loss 1.207401\n",
      "iteration 600/1000: loss 1.154891\n",
      "iteration 700/1000: loss 1.016298\n",
      "iteration 800/1000: loss 1.004904\n",
      "iteration 900/1000: loss 0.891820\n",
      "\n",
      "The 3/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.909394\n",
      "iteration 100/1000: loss 2.193496\n",
      "iteration 200/1000: loss 1.810792\n",
      "iteration 300/1000: loss 1.557878\n",
      "iteration 400/1000: loss 1.373106\n",
      "iteration 500/1000: loss 1.216788\n",
      "iteration 600/1000: loss 1.142319\n",
      "iteration 700/1000: loss 1.057474\n",
      "iteration 800/1000: loss 0.991854\n",
      "iteration 900/1000: loss 0.929470\n",
      "\n",
      "The 4/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.879010\n",
      "iteration 100/1000: loss 2.025752\n",
      "iteration 200/1000: loss 1.671603\n",
      "iteration 300/1000: loss 1.533261\n",
      "iteration 400/1000: loss 1.370864\n",
      "iteration 500/1000: loss 1.243556\n",
      "iteration 600/1000: loss 1.105855\n",
      "iteration 700/1000: loss 1.081124\n",
      "iteration 800/1000: loss 0.958427\n",
      "iteration 900/1000: loss 0.898080\n",
      "\n",
      "The 5/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.055364\n",
      "iteration 100/1000: loss 1.997846\n",
      "iteration 200/1000: loss 1.769972\n",
      "iteration 300/1000: loss 1.501075\n",
      "iteration 400/1000: loss 1.384963\n",
      "iteration 500/1000: loss 1.202469\n",
      "iteration 600/1000: loss 1.163803\n",
      "iteration 700/1000: loss 1.015710\n",
      "iteration 800/1000: loss 0.970657\n",
      "iteration 900/1000: loss 0.928193\n",
      "\n",
      "The 6/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.773469\n",
      "iteration 100/1000: loss 2.047282\n",
      "iteration 200/1000: loss 1.812404\n",
      "iteration 300/1000: loss 1.533092\n",
      "iteration 400/1000: loss 1.331504\n",
      "iteration 500/1000: loss 1.243846\n",
      "iteration 600/1000: loss 1.100757\n",
      "iteration 700/1000: loss 1.035052\n",
      "iteration 800/1000: loss 0.924969\n",
      "iteration 900/1000: loss 0.889645\n",
      "\n",
      "The 7/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.796335\n",
      "iteration 100/1000: loss 2.090108\n",
      "iteration 200/1000: loss 1.751841\n",
      "iteration 300/1000: loss 1.548845\n",
      "iteration 400/1000: loss 1.378968\n",
      "iteration 500/1000: loss 1.212860\n",
      "iteration 600/1000: loss 1.099318\n",
      "iteration 700/1000: loss 1.033299\n",
      "iteration 800/1000: loss 0.966008\n",
      "iteration 900/1000: loss 0.910208\n",
      "\n",
      "The 8/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.212820\n",
      "iteration 100/1000: loss 2.025227\n",
      "iteration 200/1000: loss 1.800799\n",
      "iteration 300/1000: loss 1.657705\n",
      "iteration 400/1000: loss 1.388773\n",
      "iteration 500/1000: loss 1.237489\n",
      "iteration 600/1000: loss 1.128014\n",
      "iteration 700/1000: loss 1.071124\n",
      "iteration 800/1000: loss 0.966412\n",
      "iteration 900/1000: loss 0.908103\n",
      "\n",
      "The 9/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.648772\n",
      "iteration 100/1000: loss 2.016935\n",
      "iteration 200/1000: loss 1.784390\n",
      "iteration 300/1000: loss 1.500378\n",
      "iteration 400/1000: loss 1.324700\n",
      "iteration 500/1000: loss 1.187788\n",
      "iteration 600/1000: loss 1.108257\n",
      "iteration 700/1000: loss 1.026943\n",
      "iteration 800/1000: loss 0.971589\n",
      "iteration 900/1000: loss 0.853312\n",
      "\n",
      "The 10/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.976867\n",
      "iteration 100/1000: loss 1.998174\n",
      "iteration 200/1000: loss 1.834519\n",
      "iteration 300/1000: loss 1.503741\n",
      "iteration 400/1000: loss 1.343425\n",
      "iteration 500/1000: loss 1.228500\n",
      "iteration 600/1000: loss 1.094948\n",
      "iteration 700/1000: loss 1.018475\n",
      "iteration 800/1000: loss 0.953284\n",
      "iteration 900/1000: loss 0.859435\n"
     ]
    }
   ],
   "source": [
    "# train 10 logistic classifier\n",
    "from linear_classifier import Logistic\n",
    "\n",
    "logistic_classifiers = []\n",
    "num_classes = np.max(y_train) + 1\n",
    "losses = []\n",
    "for i in xrange(num_classes):\n",
    "    print '\\nThe %d/%dth logistic classifier training...' % (i+1, num_classes)\n",
    "    y_train_logistic = copy.deepcopy(y_train)\n",
    "    idxs_i = y_train_logistic == i\n",
    "    y_train_logistic[idxs_i] = 1\n",
    "    y_train_logistic[-idxs_i] = 0\n",
    "    logistic = Logistic()\n",
    "    loss = logistic.train(x_train, y_train_logistic, method='sgd', batch_size=200, learning_rate=1e-6,\n",
    "              reg = 1e3, num_iters=1000, verbose=True, vectorized=True)\n",
    "    losses.append(loss)\n",
    "    logistic_classifiers.append(logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction by using trained 10 logistic classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset accuracy: 0.347367\n",
      "Validation dataset accuracy: 0.340000\n",
      "Test datast accuracy: 0.339100\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of training data and validation data\n",
    "def predict_one_vs_all(logistic_classifiers, X, num_classes):\n",
    "    scores = np.zeros((num_classes, X.shape[1]))\n",
    "    for i in xrange(num_classes):\n",
    "        logistic = logistic_classifiers[i]\n",
    "        scores[i, :] = logistic.predict(X)[1]\n",
    "    pred_X = np.argmax(scores, axis=0)\n",
    "    return pred_X\n",
    "\n",
    "pred_train_one_vs_all = predict_one_vs_all(logistic_classifiers, x_train, num_classes)\n",
    "pred_val_one_vs_all = predict_one_vs_all(logistic_classifiers, x_val, num_classes)\n",
    "pred_test_one_vs_all = predict_one_vs_all(logistic_classifiers, x_test, num_classes)\n",
    "print 'Training dataset accuracy: %f' % (np.mean(y_train == pred_train_one_vs_all))\n",
    "print 'Validation dataset accuracy: %f' % (np.mean(y_val == pred_val_one_vs_all))\n",
    "print 'Test datast accuracy: %f' % (np.mean(y_test == pred_test_one_vs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
